{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "757048c2",
      "metadata": {},
      "source": [
        "This Jupyter notebook contains code used to visualize different books using a SentenceTransformer model. The theoretical background which underpins this project as well as the actual plots we produce can be found [here](https://wandb.ai/dmeltzer/gutenberg/reports/Visualizing-Literature-using-Transformers--Vmlldzo0MTIyODEx?accessToken=1ekch7p12170nvwbtqzvy2g3shpyyboajfbalciun3ly913cdv033je1rvkoa5bj)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "63e5f440-6b1e-42be-aa91-cce887515e2c",
      "metadata": {
        "id": "63e5f440-6b1e-42be-aa91-cce887515e2c"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d6597e14",
      "metadata": {},
      "source": [
        "This notebook was written to be used on Google colab, but can be adapted to run locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ptpKvHrx5uOF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptpKvHrx5uOF",
        "outputId": "2ad1f72b-8436-4cdb-a99f-826125624d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Gutenberg\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive and cd into project directory.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Name of project directory should be changed to where ever this file is saved.\n",
        "%cd drive/My Drive/Gutenberg"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8a3bfd3b",
      "metadata": {},
      "source": [
        "Code block below installs and imports the libraries needed to process and visualize each book using transformer models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8249e7b0-b2c2-4056-bdc1-2b5d9734d288",
      "metadata": {
        "id": "8249e7b0-b2c2-4056-bdc1-2b5d9734d288"
      },
      "outputs": [],
      "source": [
        "# Install sentence-transformers to encode text.\n",
        "!pip install -U sentence-transformers\n",
        "# Install wandb to perform experiment tracking/logging.\n",
        "!pip install wandb\n",
        "\n",
        "import os\n",
        "# umap and TSNE are used to visualize data.\n",
        "import umap\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Use wandb to log/track data.\n",
        "import wandb\n",
        "# use re library to perform data cleaning.\n",
        "import re\n",
        "# books will later be saved in a pandas dataframe.\n",
        "import pandas as pd\n",
        "# plotly is used to visualize data.\n",
        "import plotly.io as pio\n",
        "# Standard import for numpy.\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# Used to track data processing.\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "59f00e30-ad6b-48c8-ac7a-1465b3cc68b1",
      "metadata": {
        "id": "59f00e30-ad6b-48c8-ac7a-1465b3cc68b1"
      },
      "source": [
        "# Functions Definitions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we define the functions needed to process the dataframe and plot the sentence embedding vectors. The books we study are scraped from ProjectGutenberg and are saved into a csv file which only contains one row and has the entirety of the book saved in the \"text\" column. The purpose of the \"process_file\" is to process each book so we have a Pandas dataframe where each row corresponds to a passage (of some fixed maximum length) and so each passage comes from a distinct chapter. Each passage is then encoded in a high-dimensional vector using a sentence transformer model.\n",
        "\n",
        "The functions \"tsne_plot\", \"umap_plot\" perform dimensional reduction using t-distributed stochastic neighbor embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP) and plot the resulting two-dimensional vectors. The function \"dist_matrix\" forms a matrix of cosine similarity distances between each chapter and \"heatmap_plot\" forms a heatmap plot from this matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f38e7a-72a2-4b87-b779-f7f364ed531a",
      "metadata": {
        "id": "f8f38e7a-72a2-4b87-b779-f7f364ed531a"
      },
      "outputs": [],
      "source": [
        "def process_file(df,model,sent_length,title):\n",
        "    \"\"\"\n",
        "    Converts a scraped book into a dataframe. Book is split by sentence length and chapters.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "    - df (Pandas Dataframe): Dataframe consisting of book scraped from Project Gutenberg. Each dataframe contains only one row corresponding to entire text of book.\n",
        "    - model (Huggingface Transformer): Transformer model which is used to encode text.\n",
        "    - sent_length (int): Splits book into passages with length of at most sent_length.\n",
        "    - title (str): Title of book.\n",
        "\n",
        "    Output:\n",
        "    - df_processed (Pandas Dataframe): Book split into passages with length at most sent_length.\n",
        "                                       Book is split such that each passage comes from a unique chapter.\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # chapt denotes current chapter. Initialize to 1\n",
        "    chapt=1\n",
        "\n",
        "    # begin and end correspond to indices for the beginning and end of the passage.\n",
        "    begin=0\n",
        "    end=sent_length\n",
        "    \n",
        "    # original dataframe, df, consists of one row where the 'Text' column contains text for entire book.\n",
        "    # process data by lowecasing text and splitting on whitespace.\n",
        "    text=df.iloc[0]['Text'].lower().split()\n",
        "    \n",
        "    # Define new processed dataframe with columns for the text, chapter number, and title of book.\n",
        "    df_processed=pd.DataFrame(columns=['text','chapter','book'])\n",
        "    \n",
        "    # Scan over entire book.\n",
        "    while begin<=len(text):\n",
        "        # Extract sentence from original text.\n",
        "        sent=text[begin:end]\n",
        "\n",
        "        # If 'chapter_end\" is in sentence we need to split the sentence so each row of the\n",
        "        # processed dataframe contains text from unique chapters.\n",
        "        if 'chapter_end' in sent:\n",
        "            # index where 'chapter_end' appears.\n",
        "            idx=sent.index('chapter_end')\n",
        "            # If idx>0 then we put all text below idx into one row of the dataframe.\n",
        "            if idx>0:\n",
        "                df_processed.loc[len(df_processed)]=[' '.join(sent[:idx]),chapt,title]\n",
        "                # if 'chapter_end' occurs before end of sent, then place all text after 'chapter_end' in a new row.\n",
        "                if idx<len(sent)-1:\n",
        "                    chapt+=1\n",
        "                    df_processed.loc[len(df_processed)]=[' '.join(sent[idx+1:]),chapt,title]\n",
        "                # If 'chapter_end' is last element of sent then we increase chapt by one and iterate to next span of text.\n",
        "                elif idx==len(sent)-1:\n",
        "                    chapt+=1\n",
        "            # if \"chapter_end\" occurs at beginning of sent then we increase chapt by one and place all following text in a new row of the dataframe.\n",
        "            elif idx==0:\n",
        "                chapt+=1\n",
        "                df_processed.loc[len(df_processed)]=[' '.join(sent[idx+1:]),chapt,title]\n",
        "        # If 'chapter_end' does not appear in sentence then we just convert sent into a string and add it to dataframe.\n",
        "        else:\n",
        "            df_processed.loc[len(df_processed)]=[' '.join(sent),chapt,title]\n",
        "        \n",
        "        # increase begin and end by sent_length to move onto next span of text.\n",
        "        begin+=sent_length\n",
        "        end+=sent_length\n",
        "        \n",
        "    sent_vecs=model.encode(df_processed['text'].to_list())\n",
        "    df_processed['embeddings']=pd.Series(list(sent_vecs))\n",
        "    \n",
        "    return df_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b25f8eea-862f-42fe-96ab-488ba3c56222",
      "metadata": {
        "id": "b25f8eea-862f-42fe-96ab-488ba3c56222"
      },
      "outputs": [],
      "source": [
        "def tsne_plot(df,\n",
        "              title=None,\n",
        "              opacity=.5,\n",
        "              size=4):\n",
        "    \n",
        "    \"\"\"\n",
        "    Makes a t-SNE plot based on the processed dataframe for each book.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "    - df (Pandas Dataframe): Dataframe of processed book split by sentence length and chapter number.\n",
        "    - title (str): Title of plot.\n",
        "    - opacity (float): Opacity of the dots in the t-SNE plot.\n",
        "    - size (float): Size of each dot in the plot.\n",
        "\n",
        "    Output:\n",
        "    - go.Figure object of t-SNE plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Form 2d t-SNE vectors using the embedding vectors for each passage in the book. \n",
        "    tsne_vecs=TSNE(n_components=2).fit_transform(np.stack(df['embeddings']))\n",
        "    \n",
        "    # Color and label each dot based on the chapter it appears in.\n",
        "    colors=df.chapter.to_numpy()\n",
        "    labels=colors\n",
        "\n",
        "    # Form a scatter plot based on the tsne vectors.\n",
        "    trace = go.Scatter(\n",
        "        x=tsne_vecs[:, 0],\n",
        "        y=tsne_vecs[:, 1],\n",
        "        text=labels,  # Specify the label for each point\n",
        "        mode='markers',\n",
        "        hoverinfo='text',  # Show label when hovering over a point\n",
        "        marker=dict(\n",
        "            size=size,\n",
        "            color=colors,\n",
        "            opacity=opacity,\n",
        "            line=dict(width=0.5, color='white'),\n",
        "            colorbar=dict(title='Colorbar Title')  # Add a color bar with title\n",
        "        )\n",
        "    )\n",
        "\n",
        "    layout = go.Layout(\n",
        "        title='TSNE Plot: '+title.capitalize() if title is not None else 'TSNE Plot',\n",
        "        hovermode='closest',\n",
        "        xaxis=dict(title='X axis'),\n",
        "        yaxis=dict(title='Y axis')\n",
        "    )\n",
        "\n",
        "    # Create a Figure object and add the trace and layout\n",
        "    fig = go.Figure(data=[trace], layout=layout)\n",
        "        \n",
        "    if title is not None:\n",
        "        pio.write_image(fig, './figures/tsne_'+title, format='png')\n",
        "    fig.show()\n",
        "    return fig\n",
        "\n",
        "def umap_plot(df,\n",
        "              title,\n",
        "              n_neighbors=10,\n",
        "              min_dist=0,\n",
        "              size=5,\n",
        "              opacity=.5,\n",
        "              save_file=True):\n",
        "    \"\"\"\n",
        "    Forms umap plot from the sentence embedding vectors in the processed dataframe.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "    - df (Pandas Dataframe): Dataframe of processed book split by sentence length and chapter number.\n",
        "    - title (str): Title of plot.\n",
        "    - n_neighbors (int): constrains size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data.\n",
        "    - min_dist (float): Controls how tightly UMAP is allowed to pack points together.\n",
        "    - size (float): Size of each dot in the umap plot.\n",
        "    - opacity (float): Opacity of each dot in the umap plot.\n",
        "    - save_file (bool): whether or not to save the umap figure.\n",
        "\n",
        "    Output:\n",
        "    - fig (go.Figure): Returns the umap plot object.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract sentence embedding vectors from the processed dataframe.\n",
        "    sent_vecs=list(df['embeddings'])\n",
        "    # Forms UMAP vectors from the sentence vectors.\n",
        "    umap_emb = umap.UMAP(n_neighbors=n_neighbors, \n",
        "                         min_dist=min_dist, \n",
        "                         metric='euclidean').fit_transform(sent_vecs)\n",
        "    \n",
        "    # Color code each sentence vector by the chapter it appears in.\n",
        "    colors=df.chapter.to_numpy()\n",
        "    \n",
        "    trace = go.Scatter(\n",
        "        x=umap_emb[:, 0],\n",
        "        y=umap_emb[:, 1],\n",
        "        text=colors,  # Specify the label for each point\n",
        "        mode='markers',\n",
        "        hoverinfo='text',  # Show label when hovering over a point\n",
        "        marker=dict(\n",
        "            size=size,\n",
        "            color=colors,\n",
        "            opacity=opacity,\n",
        "            line=dict(width=0.5, color='white'),\n",
        "            colorbar=dict(title='Colorbar Title')  # Add a color bar with title\n",
        "        )\n",
        "    )\n",
        "\n",
        "    layout = go.Layout(\n",
        "        title='UMAP: '+title.capitalize() if title is not None else 'UMAP',\n",
        "        hovermode='closest', \n",
        "        xaxis=dict(title='X axis'),\n",
        "        yaxis=dict(title='Y axis')\n",
        "    )\n",
        "\n",
        "    # Create a Figure object and add the trace and layout\n",
        "    fig = go.Figure(data=[trace], layout=layout)\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()\n",
        "    \n",
        "    if save_file is not None:\n",
        "        pio.write_image(fig, './figures/umap_'+title, format='png')\n",
        "    return fig\n",
        "\n",
        "def dist_matrix(df):\n",
        "    \"\"\"\n",
        "    Forms a matrix of cosine-similarity distances between each chapter in the book.\n",
        "\n",
        "    Input:\n",
        "    ------\n",
        "    - df (Pandas Dataframe): Dataframe of processed book split by sentence length and chapter number.\n",
        "\n",
        "    Output:\n",
        "    -------\n",
        "    - cos_distance (list): List of lists where row i and column j corresponds to cosine-distance between chapters (i+1) and (j+1).\n",
        "    \"\"\"\n",
        "\n",
        "    # Form chapter embedding vectors by taking the sum of all sentence embedding vectors in each chapter.\n",
        "    chapter_vecs=df.groupby('chapter')['embeddings'].sum().values\n",
        "    # Compute the norm of each chapter embedding vector.\n",
        "    norms=[np.sqrt(np.dot(chapter_vecs[i],chapter_vecs[i])) for i in range(len(chapter_vecs))]\n",
        "    # Compute cosine distance between each chapter embedding vector.\n",
        "    cos_distance= [[np.dot(chapter_vecs[i],chapter_vecs[j])/(norms[i]*norms[j]) \\\n",
        "                    for i in range(len(chapter_vecs))] for j in range(len(chapter_vecs))]\n",
        "    return cos_distance\n",
        "\n",
        "def heatmap_plot(df,title=None):\n",
        "    \"\"\"\n",
        "    Makes heatmap plot for the cosine-similarity distance between each chapter.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "    - df (Pandas Dataframe): Dataframe of processed book split by sentence length and chapter number.\n",
        "    - title (str): Title of heatmap plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # List of lists containing cosine distances between each chapter.\n",
        "    # Entry in row i and column j corresponds to distance between chapters (i+1) and (j+1)    \n",
        "    cos_distance= dist_matrix(df)\n",
        "    \n",
        "    # Total number of chapters in the book.\n",
        "    last_chapter=max(df['chapter'])\n",
        "\n",
        "    # Layout for heatmap plot.\n",
        "    layout = go.Layout(\n",
        "        title_text='Heatmap Plot: '+title.capitalize() if title is not None else 'heatmap Plot',\n",
        "        title_x=.5,\n",
        "        hovermode='closest', # Show distance when hovering over a point\n",
        "        xaxis=dict(title='Chapter'),\n",
        "        yaxis=dict(title='Chapter')\n",
        "    )\n",
        "    \n",
        "    # Form heatmap plot where value is cos_distance and axes are defined by the chapters.\n",
        "    data=go.Heatmap(\n",
        "        z=cos_distance,\n",
        "        x=np.arange(1,last_chapter+1),\n",
        "        y=np.arange(1,last_chapter+1),\n",
        "        colorscale='magma')\n",
        "\n",
        "    # Form go.Figure object.\n",
        "    fig = go.Figure(data=data,\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis = dict(\n",
        "            tickmode = 'linear',\n",
        "            tick0 = 1,\n",
        "            dtick = 1),\n",
        "        yaxis = dict(\n",
        "            tickmode = 'linear',\n",
        "            tick0 = 1,\n",
        "            dtick = 1))\n",
        "    return fig"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "db9bbe55-5578-4405-8954-68cfc8f2a2e2",
      "metadata": {
        "id": "db9bbe55-5578-4405-8954-68cfc8f2a2e2"
      },
      "source": [
        "# Process Books"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e63d8663",
      "metadata": {},
      "source": [
        "This section contains the code used to actually process each book.\n",
        "\n",
        "Run the code block below to define the list of books we study, the model used to form the sentence embedding vectors, and then to process each book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6-LXE99Kx5A",
      "metadata": {
        "id": "f6-LXE99Kx5A"
      },
      "outputs": [],
      "source": [
        "# List of books being studied.\n",
        "# \"Ulysses\", \"Portrait\", and \"Dubliners\" are the three major works by James Joyce in the public domain.\n",
        "# Final three entries correspond to three different translations of \"The Odyssey\" by Homer.\n",
        "# \"butcher\" refers to the translation by Butcher and Lang.\n",
        "# \"butler\" refers to translation by Samuel Butler.\n",
        "# \"pope\" refers to translation by Alexander Pope.\n",
        "books=['Ulysses','Portrait','Dubliners','butcher','butler','pope']\n",
        "\n",
        "# Use all-mpnet-base-v2 model to encode sentences since it is the most powerful sentence-transformer model available on Huggingface.\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# df_books is a dictionary where the key is the title of the book and the value is the unprocessed dataframe for each book.\n",
        "df_books={}\n",
        "\n",
        "# Read each csv file into a pandas dataframe.\n",
        "# To download each book one needs to scrape the book using the Gutenberg.py file.\n",
        "df_books['Dubliners']=pd.read_csv('./books/Dubliners_James_Joyce.csv')\n",
        "df_books['Ulysses']=pd.read_csv('./books/Ulysses_James_Joyce.csv')\n",
        "df_books['Portrait']=pd.read_csv('./books/Portrait_James_Joyce.csv')\n",
        "\n",
        "df_books['butcher']=pd.read_csv('./books/Odyssey_Homer_Butcher_Lang.csv')\n",
        "df_books['butler']=pd.read_csv('./books/Odyssey_Homer_Butler.csv')\n",
        "df_books['pope']=pd.read_csv('./books/Odyssey_Homer_Pope.csv')\n",
        "\n",
        "# Clean text for Butler's translation by removing numbers corresponding to footnotes.\n",
        "text=re.sub(r'\\[[0-9]+\\]', '', ' '.join(df_books['butler'].iloc[0]['Text'].split()))\n",
        "df_books['butler'].iloc[0]['Text']=text.lower()\n",
        "\n",
        "# df_processed is a dictionary where the key is the title of the book and the value is the processed dataframe for each book.\n",
        "df_processed={}\n",
        "\n",
        "for book in tqdm(books):\n",
        "    print(f'working on book: {book}') # Used to monitor progress of the processing of each book.\n",
        "    # If processed dataset already exists we load it from memory.\n",
        "    \n",
        "    processed_file = f'./data/df_processed_{book}'\n",
        "    \n",
        "    if os.path.exists(processed_file):\n",
        "        df_processed[book] = pd.read_csv(processed_file)\n",
        "        # ensure that the column 'embeddings' contains numpy arrays and not strings or lists.\n",
        "        df_processed[book]['embeddings']=df_processed[book]['embeddings'].map(eval)\n",
        "        df_processed[book]['embeddings']=df_processed[book]['embeddings'].map(np.array)\n",
        "        continue\n",
        "    df_processed[book]=process_file(df_books[book],model,100,book)\n",
        "    # Convert the entires in the 'embeddings' column to a numpy array.\n",
        "    df_processed[book]['embeddings']=df_processed[book]['embeddings'].map(np.array)\n",
        "    # Save the processed dataframes in a new file.\n",
        "    df_processed[book].to_csv(processed_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "69211979-33e1-4488-ae93-26ed5116b319",
      "metadata": {
        "id": "69211979-33e1-4488-ae93-26ed5116b319"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3740a1b0-6e74-48eb-af59-4ccf5d75685c",
      "metadata": {
        "id": "3740a1b0-6e74-48eb-af59-4ccf5d75685c"
      },
      "outputs": [],
      "source": [
        "# For each book we produce a t-sne, umap, and heatmap plot.\n",
        "# Each plot is then logged on wandb.\n",
        "for book in tqdm(books):\n",
        "    print(f'working on {book}')\n",
        "    \n",
        "    tsne_plt = tsne_plot(df_processed[book], title = book)\n",
        "    umap_plt = umap_plot(df_processed[book], title = book)\n",
        "    heat_matrix_plt = heatmap_plot(df_processed[book], title = book)\n",
        "    \n",
        "    with wandb.init(project='gutenberg',\n",
        "                    entity=None,\n",
        "                    job_type='EDA',\n",
        "                    name='EDA_'+book+'_plots') as run:\n",
        "\n",
        "        run.log({f\"{book}: TSNE\": tsne_plt})\n",
        "        run.log({f\"{book}: umap\": umap_plt})\n",
        "        run.log({f\"{book}: heat_matrix\": heat_matrix_plt})\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "63e5f440-6b1e-42be-aa91-cce887515e2c",
        "59f00e30-ad6b-48c8-ac7a-1465b3cc68b1",
        "y2h-E7YI6XCC"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
